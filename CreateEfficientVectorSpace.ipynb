{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:Blue'> Vector Space Model with a Champion List Implementation </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### The aim is to create an Information Retrieval model able to retrieve documents, given a 'free-form' query in input through an union of the champion lists of the single terms contained in the query.\n",
    "- #### After the first query, the user can express his preferences, then the Rocchio algorithm moves the query in the direction desired by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just some useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "from math import log, sqrt\n",
    "from functools import reduce\n",
    "import csv\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's rertrieve the corpus and store it in a dictionary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good old csv module do the job as seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDescription:\n",
    "    \n",
    "    def __init__(self,docID, title, description):\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "        self.docID = docID\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.title\n",
    "\n",
    "def readMovieDescriptions():\n",
    "    filename = 'MovieSummaries/plot_summaries.txt'\n",
    "    movie_names_file = 'MovieSummaries/movie.metadata.tsv'\n",
    "    with open(movie_names_file, 'r') as csv_file:\n",
    "        movie_names = csv.reader(csv_file, delimiter='\\t')\n",
    "        names_table = {}\n",
    "        for name in movie_names:\n",
    "            names_table[name[0]] = name[2]\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        descriptions = csv.reader(csv_file, delimiter='\\t')\n",
    "        corpus = []\n",
    "        for docID, desc in enumerate(descriptions):\n",
    "            try:\n",
    "                movie = MovieDescription(docID, names_table[desc[0]], desc[1])\n",
    "                corpus.append(movie)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = readMovieDescriptions()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mhh... we need to normalize the description of the movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good old re module do the job as seen in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    no_punctuation = re.sub(r'[^a-zA-Z\\s]+','',text)\n",
    "    downcase = no_punctuation.lower()\n",
    "    return downcase\n",
    "\n",
    "def tokenize(text):\n",
    "    text = normalize(text)\n",
    "    return list(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in corpus:\n",
    "    movie.description, movie.title = tokenize(movie.description), tokenize(movie.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create an Inverted Index and compute Term Frequency for each docID associated with the current Term\n",
    "- #### We'll need it to compute tf-idf and create the whole space vector in an efficient way (this is the hope...).\n",
    "- #### First of all we need to deal with the title and description of the movies, then create an inverted index.\n",
    "#### (For the sake of simplicity, i merged together title and description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeInvertedIndex(corpus):\n",
    "    \"\"\"\n",
    "    Each posting is not only a document id, but the term frequency\n",
    "    where the term is contained in the article.\n",
    "    \n",
    "    We are creating an entity of this type {term: {docID: termFrequency}}\n",
    "    \"\"\"\n",
    "    index = defaultdict(dict)\n",
    "    for docID, _ in enumerate(corpus):\n",
    "        for term in corpus[docID].title + corpus[docID].description:\n",
    "            try:\n",
    "                index[term][docID] += 1\n",
    "            except KeyError:\n",
    "                index[term][docID] = 1\n",
    "    return index, docID+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inv_index, length_corpus = makeInvertedIndex(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inv_index['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Inverted Index containing for each term, the 15 most relevant docIDs (sorted by Term Frequency) -> ChampionLists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeInvertedIndexChampionList(inv_index):\n",
    "\n",
    "    inv_index_champList = defaultdict(list)\n",
    "    max_length_champList = 15\n",
    "\n",
    "    \"\"\"\n",
    "    We are creating an entity of this type {term: [docID1, docID2, docID3...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    for term in inv_index:\n",
    "        sorted_dict = {docID: tf for docID, tf in sorted(inv_index[term].items(), key=lambda item: item[1], reverse=True)}\n",
    "        if len(sorted_dict) > max_length_champList:\n",
    "            for i in range(0,max_length_champList):\n",
    "                docID = list(sorted_dict.keys())[i]\n",
    "                inv_index_champList[term].append(docID)\n",
    "        else:\n",
    "            for i in range(0,len(sorted_dict)):\n",
    "                docID = list(sorted_dict.keys())[i]\n",
    "                inv_index_champList[term].append(docID)\n",
    "    return inv_index_champList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champList = makeInvertedIndexChampionList(inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champList['hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Inverted Index with the tfidf for each document of each term \n",
    "## -> {term: {docID: tfidf}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeInvertedIndex_tfidf(inv_index, length_corpus):\n",
    "    N = length_corpus\n",
    "    inv_index_tfidf = defaultdict(dict)\n",
    "    \n",
    "    for term in inv_index.keys():\n",
    "        idf = log(N/len(inv_index[term]))\n",
    "        inv_index_tfidf[term] = {docID: tf * idf for docID, tf in inv_index[term].items()}\n",
    "    return inv_index_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index_tfidf = makeInvertedIndex_tfidf(inv_index, length_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inv_index_tfidf['taxi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's investigate the term 'taxi' in the list regarding the first document in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docID = 0 ## first document\n",
    "term = 'taxi' ## first term object\n",
    "print(\"term:\", term, \"tfidf_docID_0:\", inv_index_tfidf[term][docID])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Inverted Index represent the whole Vector Space in an efficient way!!!\n",
    "- #### Represent a vector of the Vector Space for the first document -> {Term: tf-idf}\n",
    "- #### Absence of a term in the vector -> tfidf = 0 -> Compact Representation!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentToVector(docID, inv_index_tfidf):\n",
    "    vector = {}\n",
    "\n",
    "    for term in inv_index_tfidf.keys():\n",
    "        try:\n",
    "            vector[term] = inv_index_tfidf[term][docID]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docID = 0\n",
    "vector = documentToVector(docID, inv_index_tfidf)\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do better, sort by tfidf and normalize the vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortVector(vector):\n",
    "    sorted_vector = {k: v for k, v in sorted(vector.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return sorted_vector\n",
    "\n",
    "def normalizeVector(vector):\n",
    "    length = sqrt(sum([x**2 for x in vector.values()]))\n",
    "    normalized = {k: tfidf/length for k, tfidf in vector.items()}\n",
    "    return normalized\n",
    "\n",
    "def sortAndNormalize(vector):\n",
    "    return sortVector(normalizeVector(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = sortAndNormalize(vector)\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a a method to parse a query of terms in a normalized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryAsVector(query):\n",
    "    query = tokenize(query)\n",
    "    query_vector = {}\n",
    "\n",
    "    for term in query: #iterate through all the query terms\n",
    "        query_vector[term] = 1\n",
    "    query_vector = normalizeVector(query_vector)\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"christmas murder love\"\n",
    "query_vector = queryAsVector(query)\n",
    "query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the VectorSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVectorSpace(inv_index_tfidf, length_corpus):\n",
    "    vectorSpace = defaultdict(dict)\n",
    "    for term in inv_index_tfidf.keys():\n",
    "        for docID in inv_index_tfidf[term].keys():\n",
    "            vectorSpace[docID][term] = inv_index_tfidf[term][docID]\n",
    "    return vectorSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorSpace = createVectorSpace(inv_index_tfidf, length_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorSpace[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to normalize the whole Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeVectorSpace(vectorSpace):\n",
    "    for docID, vector in vectorSpace.items():\n",
    "        vectorSpace[docID] = sortAndNormalize(vectorSpace[docID])\n",
    "    return vectorSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorSpace = normalizeVectorSpace(vectorSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorSpace[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the Inverted Index of the normalized Vector Space ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeInvertedIndexNormalized(vectorSpace):\n",
    "    inv_index_normalized = defaultdict(dict)\n",
    "    for docID in vectorSpace.keys():\n",
    "        for term, tfidf_normalized in vectorSpace[docID].items():\n",
    "            inv_index_normalized[term][docID] = tfidf_normalized\n",
    "    return inv_index_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index_normalized = makeInvertedIndexNormalized(vectorSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inv_index_normalized['taxi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have all the ingredients to compute the Cosine similarity between a query represented as a vector and the Vector Space!!\n",
    "- #### Cosine similarity between normalized vectors -> Inner Product\n",
    "- #### Here an example of inner product between the query vector and another document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def innerProduct(vectorA, vectorB):\n",
    "    setA = set(vectorA.keys())\n",
    "    setB = set(vectorB.keys())\n",
    "    product = 0\n",
    "    intersection = setA.intersection(setB)\n",
    "    \n",
    "    for term in intersection:\n",
    "        product += vectorA[term] * vectorB[term]\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"christmas murder love\"\n",
    "query_vector = queryAsVector(query)\n",
    "\n",
    "vector = vectorSpace[10]\n",
    "\n",
    "innerProduct(query_vector, vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for the best answer to a given query in the whole VectorSpace... ðŸ¤–\n",
    "- Compute Inner Product between a query and every document of the vectorSpace -> Compute a search on the normalized Inverted Index!!!\n",
    "- sort results by value of the Inner Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"christmas murder love\"\n",
    "query_vector = queryAsVector(query)\n",
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_products = defaultdict(dict)\n",
    "\n",
    "for term, query_tfidf in query_vector.items():\n",
    "    for docID, tfidf_normalized in inv_index_normalized[term].items():\n",
    "        result_products[docID][term] = tfidf_normalized * query_tfidf\n",
    "for docID, vector_products in result_products.items():\n",
    "    print(docID, vector_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_innerProduct = {}\n",
    "for docID, vector_products in result_products.items():\n",
    "    result_innerProduct[docID] = sum(vector_products.values())\n",
    "result_innerProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_innerProduct = sortVector(result_innerProduct)\n",
    "result_innerProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_titles = {}\n",
    "for docID in result_innerProduct.keys():\n",
    "    result_titles[docID] = ' '.join(corpus[docID].title)\n",
    "result_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the show, wrap everything in a function!\n",
    "- #### As we can see, we have a search that has a time complexity O(#terms in query * #documents where the current term is present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchVectorSpaceAsInvertedIndex(query_vector, inv_index_normalized):\n",
    "    result_products = defaultdict(dict)\n",
    "\n",
    "    for term, query_tfidf in query_vector.items():\n",
    "        for docID, tfidf_normalized in inv_index_normalized[term].items():\n",
    "            result_products[docID][term] = tfidf_normalized * query_tfidf\n",
    "\n",
    "        result_innerProduct = {}\n",
    "        for docID, vector_products in result_products.items():\n",
    "            result_innerProduct[docID] = sum(vector_products.values())\n",
    "        result_innerProduct = sortVector(result_innerProduct)\n",
    "\n",
    "        result_titles = {}\n",
    "        for docID, inner_product in result_innerProduct.items():\n",
    "            result_titles[docID] = ' '.join(corpus[docID].title)\n",
    "    return result_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "searchVectorSpaceAsInvertedIndex(query_vector, inv_index_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -> Let's try to use the vectorSpace for the search instead!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"christmas murder love\"\n",
    "query_vector = queryAsVector(query)\n",
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_innerProduct = {}\n",
    "for docID, current_vector in vectorSpace.items():\n",
    "    inner_product = innerProduct(query_vector, current_vector)\n",
    "    if inner_product > 0:\n",
    "        result_innerProduct[docID] = inner_product\n",
    "result_innerProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_innerProduct = sortVector(result_innerProduct)\n",
    "result_innerProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_titles = {}\n",
    "for docID in result_innerProduct.keys():\n",
    "    result_titles[docID] = ' '.join(corpus[docID].title)\n",
    "result_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the show, wrap everything in a function!\n",
    "- #### As we can see, we have a search that has a time complexity O(#documents in the Vector Space * #terms contained in the current document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docIDListToTitles(result):\n",
    "    res_titles = {docID: ' '.join(corpus[docID].title) for docID in result}\n",
    "    return res_titles\n",
    "\n",
    "def searchVectorSpace(query_vector, vectorSpace):\n",
    "    result_innerProduct = {}\n",
    "    for docID, current_vector in vectorSpace.items():\n",
    "        inner_product = innerProduct(query_vector, current_vector)\n",
    "        if inner_product > 0:\n",
    "            result_innerProduct[docID] = inner_product\n",
    "    result_sorted_by_innerProduct = sortVector(result_innerProduct)\n",
    "    docID_list = list(result_sorted_by_innerProduct.keys())\n",
    "    result_titles = docIDListToTitles(docID_list)\n",
    "    return result_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "searchVectorSpace(query_vector, vectorSpace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK! Everything is working, let's work with the Champion Lists!!!\n",
    "- #### The champList entity it's of the type {term: [docID1, docID14, docID23, ...]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responding to a query using the CampionList... ðŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def union(listA, listB):\n",
    "    setA = set(listA)\n",
    "    setB = set(listB)\n",
    "    union = setA.union(setB)\n",
    "    return list(union)\n",
    "\n",
    "def searchChampionList(query, champList):\n",
    "    query = tokenize(query)\n",
    "    result_list = []\n",
    "    \n",
    "    for term in query:\n",
    "        result_list.append(champList[term])\n",
    "    union_result_list = reduce(union, result_list)\n",
    "    return docIDListToTitles(union_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"christmas murder love\"\n",
    "searchChampionList(query, champList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the ChampList and the vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveObject(obj, name):\n",
    "    with open('objects/' + name + '.pkl', 'wb') as outfile:\n",
    "        pickle.dump(obj, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def loadObject(name):\n",
    "    with open('objects/' + name + '.pkl', 'rb') as infile:\n",
    "        return pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveObject(champList, \"champList\")\n",
    "saveObject(vectorSpace, \"vectorSpace\")\n",
    "saveObject(corpus, \"corpus\")\n",
    "saveObject(inv_index_normalized, \"inv_index_normalized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
